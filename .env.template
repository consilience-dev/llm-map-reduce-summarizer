# LLM Provider API Keys
# Rename this file to .env and add your actual API keys

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=your_openai_org_id_here_if_applicable
OPENAI_MODEL=gpt-3.5-turbo  # Options: gpt-3.5-turbo, gpt-4, etc.

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-sonnet-20240229  # Options: claude-3-opus, claude-3-sonnet, etc.

# Request Configuration
MAX_CONCURRENT_REQUESTS=5  # Maximum parallel requests to LLM API
TEMPERATURE=0.3  # Temperature parameter for LLM responses (0.0-1.0)
MAX_TOKENS=1000  # Maximum tokens in LLM responses
REQUEST_TIMEOUT=60  # Timeout for API requests in seconds
RETRY_ATTEMPTS=3  # Number of retry attempts for failed requests
RETRY_DELAY=5  # Delay between retry attempts in seconds

# Default Provider Selection
DEFAULT_PROVIDER=openai  # Options: openai, anthropic
